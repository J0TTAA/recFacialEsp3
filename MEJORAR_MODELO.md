# Gu√≠a para Mejorar el Modelo de Reconocimiento Facial

## Problema Identificado

El modelo est√° dando **falsos positivos**: clasifica incorrectamente a otras personas como "me" con scores altos (ej: 0.92).

### Causas Posibles

1. **Umbral demasiado bajo**: El umbral actual (0.75) es muy permisivo
2. **Dataset desbalanceado**: 94 fotos "me" vs 200 "not_me" (31.97% vs 68.03%)
3. **Falta de diversidad**: Las fotos de "not_me" pueden no ser suficientemente variadas
4. **Overfitting**: El modelo puede estar sobreajustado a las condiciones espec√≠ficas del entrenamiento

---

## Soluci√≥n 1: Aumentar el Umbral (Soluci√≥n Inmediata) ‚ö°

### Opci√≥n A: Cambiar en el c√≥digo

Edita `api/resources/verify.py` l√≠nea 30:

```python
VERIFY_THRESHOLD = float(os.environ.get("VERIFY_THRESHOLD", 0.90))  # Cambiar de 0.75 a 0.90
```

### Opci√≥n B: Usar archivo .env

Crea un archivo `.env` en la ra√≠z del proyecto:

```env
VERIFY_THRESHOLD=0.90
```

### Opci√≥n C: Usar el umbral √≥ptimo de la evaluaci√≥n

Seg√∫n `evaluate.py`, el umbral √≥ptimo es **0.99999**:

```env
VERIFY_THRESHOLD=0.95
```

**Recomendaci√≥n**: Empieza con **0.90** y ajusta seg√∫n resultados.

---

## Soluci√≥n 2: Mejorar el Dataset de Entrenamiento

### 2.1 Aumentar variedad en "not_me"

**Problema**: Solo 200 fotos de "not_me" pueden no ser suficientes.

**Soluciones**:
- Agregar m√°s fotos de personas diferentes en `data/not_me/`
- Variedad en:
  - Edades diferentes
  - G√©neros diferentes
  - Etnias diferentes
  - Expresiones faciales variadas
  - Iluminaciones diferentes
  - √Ångulos diferentes

**Recomendaci√≥n**: M√≠nimo 300-400 fotos de "not_me"

### 2.2 Aumentar variedad en "me"

**Problema**: 94 fotos tuyas pueden no cubrir todas las variaciones.

**Soluciones**:
- Agregar m√°s fotos tuyas en `data/me/`:
  - Diferentes √°ngulos (frontal, perfil, 3/4)
  - Diferentes expresiones (sonriendo, serio, etc.)
  - Diferentes iluminaciones
  - Con/sin gafas, barba, etc.
  - Diferentes edades (si tienes fotos antiguas)

**Recomendaci√≥n**: M√≠nimo 150-200 fotos de "me"

### 2.3 Balancear el dataset

**Ratio ideal**: 1:1 o m√°ximo 1:2 (me:not_me)

**Actual**: 94:200 (1:2.13) - Aceptable pero mejorable

**Objetivo**: 150:300 (1:2) o mejor a√∫n 200:200 (1:1)

---

## Soluci√≥n 3: Reentrenar el Modelo

Despu√©s de mejorar el dataset, reentrena:

```bash
# 1. Recortar nuevas caras
python scripts/crop_faces.py

# 2. Generar nuevos embeddings
python scripts/embeddings.py

# 3. Reentrenar el modelo
python train.py

# 4. Reevaluar
python evaluate.py
```

---

## Soluci√≥n 4: Ajustar Hiperpar√°metros del Modelo

### Opci√≥n A: Modificar LogisticRegression

Edita `train.py` l√≠nea 95-101:

```python
model = LogisticRegression(
    max_iter=500,              # Aumentar iteraciones
    class_weight='balanced',   # Mantener balanceado
    random_state=RANDOM_SEED,
    solver='liblinear',
    C=0.1,                     # A√ëADIR: Regularizaci√≥n m√°s fuerte (default es 1.0)
    penalty='l2'               # A√ëADIR: Regularizaci√≥n L2 expl√≠cita
)
```

**Par√°metro C**:
- **C < 1.0**: M√°s regularizaci√≥n (m√°s conservador, menos overfitting)
- **C = 1.0**: Default
- **C > 1.0**: Menos regularizaci√≥n (m√°s flexible)

**Recomendaci√≥n**: Probar con `C=0.1` o `C=0.5`

### Opci√≥n B: Usar SVM en lugar de LogisticRegression

SVM (Support Vector Machine) puede ser m√°s robusto:

```python
from sklearn.svm import SVC

model = SVC(
    probability=True,          # Necesario para predict_proba
    class_weight='balanced',
    kernel='rbf',
    C=1.0,
    gamma='scale'
)
```

---

## Soluci√≥n 5: Data Augmentation (Aumento de Datos)

Agregar variaciones de tus im√°genes existentes:

### Script de aumento de datos

```python
# scripts/augment_data.py
from PIL import Image, ImageEnhance, ImageFilter
import os
from glob import glob

def augment_image(image_path, output_dir):
    """Crea variaciones de una imagen."""
    img = Image.open(image_path)
    base_name = os.path.splitext(os.path.basename(image_path))[0]
    
    # 1. Rotaci√≥n ligera
    img.rotate(5).save(os.path.join(output_dir, f"{base_name}_rot5.jpg"))
    img.rotate(-5).save(os.path.join(output_dir, f"{base_name}_rot-5.jpg"))
    
    # 2. Brillo
    enhancer = ImageEnhance.Brightness(img)
    enhancer.enhance(0.8).save(os.path.join(output_dir, f"{base_name}_dark.jpg"))
    enhancer.enhance(1.2).save(os.path.join(output_dir, f"{base_name}_bright.jpg"))
    
    # 3. Contraste
    enhancer = ImageEnhance.Contrast(img)
    enhancer.enhance(0.8).save(os.path.join(output_dir, f"{base_name}_lowcontrast.jpg"))
    enhancer.enhance(1.2).save(os.path.join(output_dir, f"{base_name}_highcontrast.jpg"))
```

---

## Plan de Acci√≥n Recomendado

### Paso 1: Soluci√≥n Inmediata (5 minutos)
1. Aumentar umbral a **0.90** o **0.95**
2. Reiniciar la API
3. Probar de nuevo

### Paso 2: Mejora del Dataset (1-2 horas)
1. Agregar m√°s fotos de "not_me" (m√≠nimo 100-200 adicionales)
2. Agregar m√°s fotos tuyas variadas (m√≠nimo 50-100 adicionales)
3. Asegurar variedad en condiciones, √°ngulos, expresiones

### Paso 3: Reentrenamiento (10-15 minutos)
1. Recortar caras: `python scripts/crop_faces.py`
2. Generar embeddings: `python scripts/embeddings.py`
3. Entrenar: `python train.py`
4. Evaluar: `python evaluate.py`

### Paso 4: Ajuste Fino (Opcional)
1. Si persisten problemas, ajustar hiperpar√°metros
2. Probar con regularizaci√≥n m√°s fuerte (C=0.1)
3. Considerar SVM como alternativa

---

## Umbrales Recomendados

| Nivel de Seguridad | Umbral | Descripci√≥n |
|-------------------|--------|-------------|
| **Bajo** | 0.75 | Permisivo, m√°s falsos positivos |
| **Medio** | 0.85 | Balanceado |
| **Alto** | 0.90 | Estricto, menos falsos positivos |
| **Muy Alto** | 0.95 | Muy estricto, m√°xima seguridad |
| **Extremo** | 0.99 | Extremadamente estricto |

**Recomendaci√≥n inicial**: **0.90**

---

## Verificaci√≥n de Mejoras

Despu√©s de aplicar cambios, verifica:

1. **Probar con fotos tuyas**: Deber√≠a dar `is_me: true` con score > umbral
2. **Probar con fotos de otras personas**: Deber√≠a dar `is_me: false` con score < umbral
3. **Probar casos l√≠mite**: Fotos que se parezcan a ti pero no sean tuyas
4. **Revisar m√©tricas**: Ejecutar `evaluate.py` y verificar m√©tricas

---

## Monitoreo Continuo

Despu√©s de desplegar:

1. **Logging**: Revisar logs JSON para identificar patrones
2. **Falsos positivos**: Registrar casos donde el modelo falla
3. **Falsos negativos**: Registrar casos donde no te reconoce
4. **Retraining**: Reentrenar peri√≥dicamente con nuevos datos

---

## Notas Importantes

‚ö†Ô∏è **El modelo perfecto (100% accuracy) puede ser signo de overfitting**
- Si el modelo tiene 100% en validaci√≥n pero falla en producci√≥n, est√° sobreajustado
- Esto es com√∫n cuando el dataset es peque√±o o poco variado

‚úÖ **Es normal que el modelo no sea 100% perfecto**
- 95-98% de accuracy es excelente para reconocimiento facial
- Lo importante es minimizar falsos positivos (seguridad)

üìä **Balance entre seguridad y usabilidad**
- Umbral alto = M√°s seguro pero puede rechazar casos v√°lidos
- Umbral bajo = M√°s permisivo pero puede aceptar casos inv√°lidos
- Encuentra el balance seg√∫n tu caso de uso

